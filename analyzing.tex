\section{Introduction}

The goal of this chapter is to show how {\bibtex} files are analyzed:
what should be done about {\bibtex} in principle and practice
\chapref{sec:analyzing_what_to_do} and an analyzing prototype named
{\orangutan} \chapref{sec:analyzing_orangutan}.


\section{What should be done about {\bibtex}}
\label{sec:analyzing_what_to_do}
\subsection{In principle}

Due to the practical issues in changing or replacing {\bibtex} and to
ensure separation of concerns, an analyzing tool should be an
augmenting tool.

To analyze {\bibtex}, one would need to parse a {\bibtex} file into a
suitable representation.  This representation would need to parse
{\bibtex} entries and strings at a minimum.  If one intents to
\newdef{pretty print}, \ie, writing a {\bibtex} file formatted
consistently in human readable way, the result after resolving all
issues, the parsing also needs the preambles.  Comments are
technically optional, but should be kept too.

The easiest approach is a two step parse: first taking care of the
lexical and correctness concerns, then the consistency concerns.  By
taking care of the lexical and correctness concerns first, the optimal
conditions for taking care of the consistency concerns is made.

The term database is used for both local and online databases, since
it does not matter if one uses a local copy, if it is kept consistent
with the online database.

\subsubsection{First step: Lexical and correctness}
\begin{itemize}
\item Spelling errors in general can be detected either by online
  lookups or a spell checker.  A combination of the two might be an
  improvement, since it gives a potential indicator of false
  positives.  Using a spell checker requires a way to configure the
  language for the individual entries.

\item Spelling errors in names should use a database of names for
  detection of misspellings.

\item Initials can be detected by finding cases with a single letter,
  perhaps followed by a punctuation.  For suggestions, databases with
  names will be useful.  Handling multiple letters combined can be
  done by looking for relatively few (probably up to 3) letters that
  are all capitalized.  Another approach is to use a database to
  detect initials and their corresponding full names.

\item Erroneous online lookups will be impossible to detect with
  certainty, because one's intended search result cannot be known by
  software.  The detection of the other issues can give an indicator
  of a bad lookup.  Since online lookups is a likely way to handle the
  databases needed for this tool, the quality of a lookup is a
  concern.  The most trusted online database available should be used
  whenever possible.  A system doing lookups in multiple databases
  will give further indications of potential issues.  The results
  should always be confirmed by the user, to prevent erroneous data.

\item Conformity to de-facto standards and the {\bibtex} specification
  requires a set of rules specifying: required tags, optional tags,
  exclusive tags, and inclusive tags.  The values should be validated
  to the extent possible.

\item Detecting invalid values should be done when clear rules for
  correct values can be specified: such as ISSN, year, and month.
  Furthermore, values that can be verified using a database should
  also be verified.

\item Detection of journal names in abbreviated form, or in full form,
  can be done using a database of known journal names and their
  abbreviations.  It can use a database to de-abbreviate or
  abbreviate.  Furthermore, this can be refined by detecting unknown
  abbreviations.

\item To handle {\bibtex} strings that end up as part of the text, the
  text should be compared with the strings in the \file{bib}.
\end{itemize}


\subsubsection{Second step: Consistency}
\begin{itemize}
\item To detect duplicate entries, the best way is to: first compare
  unique identifiers, if the identifiers are missing, then identical
  entries.  Otherwise, potential duplicates are detected by a
  specification of similarity: having the title and authors as primary
  indicators.  For duplicated values, it should use the strings if
  possible, to determine when the same source is referenced.  For the
  duplicated values, it should maintain a list of values that are
  likely to be repeated (such as journal names) and use that to detect
  duplicates in textual content.

\item To detect name changes of forums, a database of known changes is
  used, in conjunction with a way of specifying the changes.

\item Inconsistent tag usage should be handled by comparing entries
  from the same forums.  The comparison should show missing and
  additional tags.  Relating forum entries from different points in
  time might increase the usefulness, but adds the need to handle
  changes over time.

\item Inconsistent entry keys should be handled by having a naming
  scheme based on the data in the entries, with a way to disambiguate
  if two different entries would get the same name.
\end{itemize}


\subsubsection{Configuration}
\label{sec:analyzing_configuration}

Both of the steps above require ways to configure the behavior, to
prevent false positives.  The use of configurations should be as close
to {\bibtex} as possible.

The preference for using the {\bibtex} format allows people to use
what they are familiar with.  Using a format that is readily supported
in programming frameworks, \eg, JSON or XML, might be easier by the
implementer.  However, people outside computer science, such as a
physicist or the helpful chemist from earlier, will likely not be
familiar with such formats, nor should they required to.  A user of
{\bibtex} should at best only be concerned with the {\bibtex} format,
when working with {\bibtex}.

``To use something in anger'', is an idiom for: when something has
been tested in practice.  The idea of coding in anger has been
expressed by Philip Wadler for functional
programming~\cite{wadler1997_functional}.  For the {\bibtex} user,
coding in anger could be the situation where the deadline is getting
closer and one just needs things to work, at best ten minutes ago.
When faced with frustrations like that, one tends not to care about
beautiful and elegant solutions, rather wanting something that works
with a minimum of personal effort.  To accommodate the user writing
{\bibtex} in anger is another reason to keep the specification as
close to {\bibtex} as possible, since it will minimize the effort.

Configuration should be done via de-facto standards inside the
\file{bib}, whenever possible.  For some configurations, de-facto
standards inside the \file{bib} are unreasonable.  These
configurations are better put into separate files.  However, the
specification should still be designed to match {\bibtex} as closely
as possible, \ie, still using entries, tags and values to configure.


\subsection{In practice}
\label{sec:analyzing_in_practice}

\subsubsection{Entry level configurations}

For entry level configuration, it is appropriate to introduce two
de-facto standards: \texttt{OLDforum} to mark a previous name of a
forum, and \texttt{OPTanalyze} to configure the analysis.

The division into two de-facto standards is done for two reasons: for
\texttt{OLDforum}, the additional standard will allow bibliography
styles to make use of the additional information (one could easily
imagine a bibliographic style write ``\texttt{NISSC \textit{formerly
    known as} NCSC}''), and for \texttt{OPTanalyze}, the content is
considered unlikely to be relevant to print in a bibliography.
Furthermore, the settings for \texttt{OPTanalyze} are kept in one tag
to prevent a multitude of new tags.

For the \texttt{OLDforum} tag, the value should be the string
containing the old name for the forum.  In some cases, a forum can
have multiple name changes.  When multiple name changes occur,
referring to the most recent name in the \file{bib} is desired.  The
tag is intended for disambiguation within a given file, not all files
in general.  If the tag is used for a bibliography style, having a
multitude of names as the value will likely be confusing.

However, if a forum name has been omitted, because no entries with
that name were in the \file{bib}, then a re-detection of name changes
will be needed if this name is added later.  Furthermore, using the
old forum name in bibliography styles may cause issues if we only
desire to show the previous name, when it is used in the references.
Since the tag is presently not in use in any bibliography style, these
issues are not considered further.  Alternatively, the format could be
a list of names (comma separated, since {\bibtex} uses a comma as a
separator), this list would allow detailed backtracking.

Defining entry level deviations is done using \texttt{OPTanalyze},
using spaces to separate multiple settings.  The configurations is
written, so each option can be read stand alone.  The values for
desired deviations are as follows:

\begin{itemize}
\item \texttt{@DUPLICATEOK=tag} to specify that a potential duplicate
  is deliberate, replacing \texttt{tag} with the entry key of the
  potential duplicate.
\item \texttt{@LANG=XX} to specify the desired spell check language,
  replacing \texttt{XX} with the language code desired, \eg.,
  \texttt{@LANG=FR}.
\item \texttt{@SPELLINGOK} to mark the spelling as correct.
\item \texttt{@AUTHORSPELLINGOK} to mark that the names are correct.
\item \texttt{@AUTHORINITIALSOK} to mark that the initials are correct.
\item \texttt{@NOLOOKUP} to mark that no lookup should be done for
  the content of this entry.
\item \texttt{@CONFORMITYOK} to mark conformity to the specification
  and de-facto standards as correct.
\item \texttt{@ABBREVIATIONOK} to mark an abbreviated form as correct.
\item \texttt{@STRINGSOK} to mark that the text does not contain any
  strings.
\item \texttt{@TAGSOK=forum} to mark that the usage of tags is correct and
  defines the standard for tag use for the entries from the \emph{same
  occurrence} of the forum.
\item \texttt{@TAGSOK=future} to mark that the usage of tags is
  correct and defines the standard for tag use for the entries from
  the \emph{same and future occurrences} of the forum.
\item \texttt{@TAGSOK=single} to mark that the usage of tags is
  correct for this single entry, not affecting other entries from the
  forum.
\item \texttt{@ENTRYKEYOK} to mark the entry key as correct.
\item \texttt{@LEXICALLYOK} to ignore all lexical checks for the
  entry.  Should be used with care.
\item \texttt{@CONSISTENCYOK} to ignore all consistency checks for the
  entry.  Should be used with care.
\item \texttt{@OK}, to mark an entry as fully correct, essentially the
  same as marking the entry with: ``\texttt{@CONFORMITYOK @LEXICALLYOK
    @CONSISTENCYOK}''.  Should be used with care.
\end{itemize}

The settings: \texttt{@ABBREVIATIONOK}, \texttt{@LEXICALLYOK},
\texttt{@CONSISTENCYOK}, and \texttt{@OK} are mainly present for
convenience, and to ensure that the configuration is consistent.  An
example of the configurations can be seen in
\figref{fig:analyzing_added_de_facto_standards}.

\begin{figure}
  \centering
\begin{verbatim}
@BOOK{blendstrup1994Mistbaenk,
  author = "Jens Blendstrup",
  title = "Mennesker i En Mistb{\ae}nk",
  year = 1994,
  OPTanalyze = "@LANG=DA @AUTHORSPELLINGOK @CONFORMITYOK"
}
\end{verbatim}
  \caption{An example using the de-facto standards for configuration,
    setting the spell checking language to Danish, accepting the name
    ``Jens Blendstrup'' and ignoring the missing publisher.}
  \label{fig:analyzing_added_de_facto_standards}
\end{figure}

\remark{Consider reformulating}

For the conformity and de-facto analysis, a consideration would be to
have configurations, for explicitly specifying which deviations are
accepted.  For instance, specifying that a missing title is accepted
in an entry.  However, explicitly allowing and denying tags, will
likely be redundant, since once a deviation has been accepted, it will
not be likely to change.

A similar set of tags could also be defined for the consistency check.
For the consistency check, it can be argued that an entry that is not
conforming to the standards of a forum, may be updated to do so.  For
instance, if the norm for a forum is to have an ISSN on all entries
and some of the entries do not have an ISSN.  Those entries might get
an ISSN assigned later, which is now possible to add to the entry.
After adding the missing information, the \texttt{@CONSISTENCYOK}
configuration is redundant.  When adding a tag, if is is the reason
for the configuration \texttt{@CONSISTENCYOK}, then one can remove
\texttt{@CONSISTENCYOK}.  Another approach to maintaining the minimal
set of configurations is to analyze whether each configurations is
necessary.  This approach is considered more appropriate, because of
the simplicity for the user.

Another potential change is to have a configuration for trusted
lookup services.  For example, if one knows that an entry is correct
in a certain database, then specifying that this database is trusted
for that entry.  This configuration might lead to a false sense of
security, since there is no way to guarantee that the data will never
be corrupted in that database.  In some cases the database will be the
definition for the correct value, \eg, for a DOI number the number
from International DOI Foundation, is by definition the correct
number.

To make the configurations even more intuitive for a {\bibtex} user,
an option is to add {\bibtex} strings with the relevant options in the
top of one's \file{bib}.  That way, when configuring, one can just use
the {\bibtex} strings and concatenate the relevant configurations.  An
example configuration using strings is displayed
\figref{fig:analyzing_added_de_facto_standards_strings}.  These
strings would have to be added in the top of the \file{bib}, so
{\bibtex} will not complain.

\begin{figure}
  \centering
\begin{verbatim}
@STRING{analyze_lang_danish = "@LANG=DA "}
@STRING{analyze_author_spelling_ok = "@AUTHORSPELLINGOK "}
@STRING{analyze_conformity_ok = "@CONFORMITYOK "}

@BOOK{blendstrup1994Mistbaenk,
  author = "Jens Blendstrup",
  title = "Mennesker i En Mistb{\ae}nk",
  year = 1994,
  OPTanalyze = analyze_lang_danish
             # analyze_author_spelling_ok
             # analyze_conformity_ok
}
\end{verbatim}
  \caption{\figref{fig:analyzing_added_de_facto_standards_strings}
    rewritten to use strings for configuration.}
  \label{fig:analyzing_added_de_facto_standards_strings}
\end{figure}


\subsubsection{Bibliography level configurations}

Some configurations are not specific to an entry, but the entire
bibliography.  Two options for these configurations are: to put such
options inside one's \file{bib}, or to put them in separate files.
Adding them to one's \file{bib} will introduce an additional mess in
the file, which is counterproductive, since the goal is to clean up
the mess.  Furthermore, the configurations may not correspond to
proper {\bibtex} formatting.  Having the configurations in separate
files provides: separation of concerns, a clean \file{bib} and allows
deviations from {\bibtex}, if needed.  Furthermore, having
configurations in separate files allows sharing of the files, for
instance, if a publisher wants their authors to follow a certain
setup.

Such configuration files should still, if possible, follow the
{\bibtex} format.  Thus, using entries with tags and values for
configuration and for configurations where entries and tags do not
make sense, defining {\bibtex} strings will be the favored choice.

Because the format described can be put into one file rather than
many, there are a few issues with disambiguation.  For instance, there
can be multiple configurations for the title of a
\texttt{@PROCEEDINGS}.  To solve this, the first word in the entry
keys should identify the type of configuration, followed by an
underscore and a descriptive name of the user's choice.  For example,
the key \texttt{forum\_ncsc}, seen in
\figref{fig:analyzing_configuration_name_change}, indicates that the
configuration is regarding the forum (more specifically a name
change).  The key prefixes are: for forum configurations
\texttt{forum}, for de-facto standards \texttt{standards} and for
validation of values \texttt{validation}.  Using this scheme allows a
normal {\bibtex} parser to read the files.

For changes in names of forums, a database of such changes is needed.
Currently, no such database exists (to the authors knowledge), so the
user will need a way to specify his own.  Even if such a database did
exist, a way to configure name changes is still desired, since the
database might not be complete.  A name change can be specified by
having two entries for the desired forum, adding the \texttt{OLDforum}
tag, marking the name change.  An example of a name change
configuration can be seen in
\figref{fig:analyzing_configuration_name_change}.

\begin{figure}
  \centering
\begin{verbatim}
@PROCEEDINGS{forum_ncsc,
  title = "National Computer Security Conference"
}

@PROCEEDINGS{forum_nissc,
  title = "National Information Systems Security Conference",
  OLDforum = "ncsc_forum"
}
\end{verbatim}
  \caption{Configuring a name change of a forum}
  \label{fig:analyzing_configuration_name_change}
\end{figure}

However, this configuration ignores that the names in the actual
\file{bib} may be in their abbreviated form.  Some forums also have,
as part of the name, text identifying which instance of the forum it
is.  For example, in \figref{fig:analyzing_configuration_name_change},
an entry would be named something like \texttt{Proceedings of the 20th
  National Information Systems Security Conference} and not just
\texttt{National Information Systems Security Conference}, as in the
example.  Instead of writing the name as a text, a better way might be
to use strings.  If the \file{bib} construct forum names by using
strings, as in
\figref{fig:analyzing_configuration_name_change_bib_file_strings}.
the configuration can reuse the string for identifying the forum
(\texttt{nissc} in the example).  This configuration will allow the
analysis to detect that it is the same string, and thus enable it to
detect that it is the same forum.  The corresponding configuration
will look like
\figref{fig:analyzing_configuration_name_change_config_file_strings}.

\begin{figure}
  \centering
\begin{small}
\begin{verbatim}
% Re-usable strings
@STRING{PROCintro = "Proceedings of the"}
@STRING{nissc = "National Information Systems Security Conference"}

% Conferences
@STRING{nissc20 = PROCintro # "20th" # nissc}

% Proceedings
@INPROCEEDINGS{porras1997emerald,
  title = "EMERALD: Event monitoring enabling response " #
          "to anomalous live disturbances",
  author = "Porras, Phillip A and Neumann, Peter G",
  booktitle = nissc20
}
\end{verbatim}
\end{small}
  \caption{\file{bib} using strings for conference names}
  \label{fig:analyzing_configuration_name_change_bib_file_strings}
\end{figure}

\begin{figure}
  \centering
\begin{verbatim}
@PROCEEDINGS{forum_nissc,
  title = nissc,
  OLDforum = "forum_ncsc"
}
\end{verbatim}
  \caption{\file{bib} using strings for conference names}
  \label{fig:analyzing_configuration_name_change_config_file_strings}
\end{figure}

\remark{Olivier: The following paragraph feels weak to me, do you have
  a suggestion for how to explain this in a clearer way?}

The configuration of name changes should be using the most general
entry type available, such as \texttt{@ARTICLE}, \texttt{@PROCEEDINGS}
and \texttt{@BOOK}.  The name change analysis recognizes and maps the
general entry types to the specific types.  For instance, recognizing
that \texttt{booktitle} in \texttt{@INPROCEEDINGS} corresponds to the
\texttt{title} in a \texttt{@PROCEEDINGS}.

Specifying the de-facto standards is done using {\bibtex} entries, and
only deviations should be specified.  The configurations correspond to
the rules in Section~\ref{sec:about_micro_use}, with the addition of
the option to refuse a tag.  The configurations are:

\begin{itemize}
\item \texttt{@REQUIRED} for a tag that is required to be present in
  the entry type.
\item \texttt{@OPTIONAL} for optional tags.
\item \texttt{@DENY} for tags that are in the default configuration,
  that we want to reject.
\item \texttt{@EXLUDES=tag} for a tag that excludes the use of another
  tag, replacing \texttt{tag} with the name of another tag.  For
  example, if one allows both \texttt{ISSN} and \texttt{DOI} as tags,
  but wants to ensure that only one of the tags is present, one would
  have the following: \texttt{ISSN = "@REQUIRED @EXLUDES=DOI"} and
  \texttt{DOI = "@REQUIRED @EXCLUDES=ISSN"}.
\item \texttt{@INCLUDES=tag} for tags where one of them is required
  and the other optional.  Usage is similar to \texttt{@EXCLUDES=tag}.
\end{itemize}

An example of a configuration of standards can be seen in
\figref{fig:analyzing_standards_config}.  This example sets article
entries to: reject \texttt{address} tags, that either \texttt{DOI} or
\texttt{ISSN} is present (but not both) and adds \texttt{url} as an
optional tag.  For book entries, the example sets: that ISBN10 and/or
ISBN13 must be present.

\begin{figure}
  \centering
\begin{verbatim}
@ARTICLE{standards_article,
  address = "@DENY",
  DOI = "@REQUIRED @EXCLUDES=ISSN",
  ISSN = "@REQUIRED @EXCLUDES=DOI",
  url = "@OPTIONAL"
}

@BOOK{standards_book,
  ISBN10 = "@REQUIRED @INCLUSIVE=ISBN13",
  ISBN13 = "@REQUIRED @INCLUSIVE=ISBN10"
}
\end{verbatim}
  \caption{A snippet of the desired {\bibtex} based configuration for
    the correctness checker}
  \label{fig:analyzing_standards_config}
\end{figure}

The configuration of standards also allows usage of a \texttt{*} as a
\newdef{wildcard}.  The wildcard will then match anything, for example
\texttt{@*PROCEEDINGS} will match \texttt{@PROCEEDINGS},
\texttt{@INPROCEEDINGS} and any entry type that one introduces that has
a name ending in proceedings.  These wildcards can be used for both
tag names and entry types.

For validation of values, some kind of valid patterns are required.
In general regular expressions can be used for validation.  Using
those, one could introduce a set of entries for validation rules, just
as done for the de-facto standards, having the patterns as the values.
However, using regular expressions contradicts the desire to keep the
configuration close to the {\bibtex} specification.

To remedy this, using strings with common patterns can be used.  For
instance, having a string named: \texttt{LETTERS\_ONLY},
\texttt{NUMBER}, \texttt{ISSN} and so on.  This use of strings will
keep it close to the {\bibtex} specification, allowing the flexibility
of regular expressions.  Using strings containing regular expressions
is done to allow adding more complicated rules than just the
predefined strings, and one can imagine sharing of useful collections
of validation strings.  An example of such a configuration can be seen
in \figref{fig:analyzing_validation_config}.

\begin{figure}
  \centering
\begin{verbatim}
@*{validation_all,
  author = LETTERS_ONLY,
  year = NUMBER
}

@ARTICLE{validation_article,
  DOI = DOI,
  ISSN = ISSN,
  url = URL
}
\end{verbatim}
  \caption{A snippet of the desired {\bibtex} based configuration for
    the correctness checker}
  \label{fig:analyzing_validation_config}
\end{figure}

Abbreviations of journal names can be configured by adding
\texttt{@ARTICLE} entries, using the two tags: \texttt{abbreviated}
and \texttt{fullname}, specifying the abbreviated journal name and
full journal name respectively.

% Bah, proceedings are not as easy ><
\remark{Still need to specify something for consistency changes and if
  a tag is the desired consistency... If the one in OPTanalyze doesn't
  actually cover this sufficiently?}

The specification of the format for entry keys is done by using a
{\bibtex} string named \texttt{ENTRY\_KEY}.  Inside
the string, some way of specifying the desired template for entry keys
is needed.  Using a template scheme, such as \texttt{\{tag\}} to match
tags, is probably the best solution.  This template system contradicts
the desire to keep the format close to {\bibtex}, but no better way
has been found.  A template could look like:
\texttt{\{author\}\{year\}\{title\}}.

However, this template is insufficient for two reasons: spaces are not
allowed in the entry key, and when people name entries, they often use
names such as: the last name of the first author in the list and one
significant word from the title.  Refining the templates to allow
\newdef{selectors} before the fields, such as: selecting the first
part of an entry, last name, first name, and significant word, will
improve the usability.  For example, \texttt{[lastname]\{author\}}
would select the last name of the authors.  One can use these
selectors in conjunction, to refine the result.  For instance, to get
the last name of the first author:
\texttt{[lastname][first]\{author\}}.  Again this moves the format
away from how {\bibtex} is defined and no better solution has been
found.

Fortunately, {\bibtex} strings comes to the rescue - at least
partially.  Having strings for the most common matches will ensure
that most users will never need to see, nor even know about, the
underlying pattern matching system.  Using strings will allow the user
to use concatenations to build the desired pattern.  And introducing
an empty string named \texttt{OF} and one named \texttt{THEN} to
support a more natural language.  An example can be seen in
\figref{fig:analyzing_entry_key_pattern}.

\begin{figure}
  \centering
\begin{small}
\begin{verbatim}
@STRING{ENTRY_KEY = LASTNAME # OF # FIRST # AUTHOR # THEN # YEAR}
\end{verbatim}
\end{small}
\caption{An example of a entry key pattern using the first name of
    the first year, followed by the year.}
\label{fig:analyzing_entry_key_pattern}
\end{figure}

The strings and selectors are:

\begin{itemize}
\item \texttt{FIRST} is the first part of a tag value, if the tag data
  is separated by \texttt{and}, like a list of authors, the first part
  of this list should be selected, otherwise select the first word.
  The corresponding selector \texttt{[first]}.
\item \texttt{LAST} is the last part of a tag value, if the tag data
  is separated by \texttt{and}, like a list of authors, the last part
  of this list should be selected, otherwise select the last word.
  The corresponding selector \texttt{[last]}.
\item \texttt{FIRSTPART} selects the first part of a tag value, if the
  tag data is separated by \texttt{and}, like a list of authors, the
  first word in each part of the list should be selected, otherwise
  just the first word.  The corresponding selector
  \texttt{[firstpart]}.
\item \texttt{LASTPART} selects the last part of a tag value, if the
  tag data is separated by \texttt{and}, like a list of authors, the
  last word in each part of the list should be selected, otherwise
  just the last word.  The corresponding selector \texttt{[lastpart]}.
\item \texttt{FIRSTNAME} same as \texttt{FIRSTPART}, included to allow
  a more natural language.
\item \texttt{LASTNAME} same as \texttt{LASTPART}, included to allow
  a more natural language.
\item \texttt{SIGNIFICANT} selects the significant words of a
  sentence, by removing anything that is not nouns.  The
  corresponding selector \texttt{[significant]}.  \remark{Olivier:
    is there a better choice?}
\item \texttt{OF} and \texttt{THEN} are empty placeholders, included
  to allow a more natural language.
\end{itemize}

The templates for tags are:

\begin{itemize}
\item \texttt{AUTHOR} is the content of the author or editor tag.  The
  corresponding template \texttt{[author]}
\item \texttt{FORUM} is the tag containing the relevant forum, such
  as: journal name, conference name and publisher.  The corresponding
  template \texttt{[forum]}.
\item \texttt{TITLE} is the content of the title tag.  The
  corresponding template \texttt{[title]}
\item \texttt{YEAR} is the content of the year tag.  The corresponding
  template \texttt{[year]}
\end{itemize}

One can introduce new tags in this manner and define appropriate
strings.  A way of expanding the selectors is desired, but would
complicate things.  Rules for handling empty tags and alternative
actions would be useful, however, this would complicate things even
further.


\section{{\orangutan}}
\label{sec:analyzing_orangutan}
\subsection{Introduction}

We have created a prototype for some of the analysis, under the name
\newdef{\orangutan}.  The name {\orangutan} is inspired from Terry
Pratchett's Discworld books, where the librarian at the Unseen
University is an orangutan.


\subsection{Why {\orangutan} came to be}

There are a lot of tools that provide partial solutions, such as using
online lookups to compare entries and to detect duplicate entries.
None of these use the idea of providing a general framework for reuse,
or de-facto configuration inside a {\bibtex} file.  {\orangutan} is a
proof of concept that analysis can be done using the de-facto
configurations.

\remark{Olivier: Feels a bit like over-selling it here}


\subsection{What is {\orangutan}}

In the same spirit as {\bibtex}, {\orangutan} is designed to be a
simple software tool to help one improve bibliographic references.
The analysis is designed over the same principles as in
Section~\ref{sec:analyzing_what_to_do}.


\subsection{How {\orangutan} is used in principle}

When analyzing {\bibtex} files, {\orangutan} operates on the first
step of the analysis: correctness and lexical concerns.  The tool uses
options, set by introducing a de-facto standard and JSON files.  The
options are for specifying the language for the spell check, entries
that are considered to be correct, and the standards used.


\subsection{How {\orangutan} is used in practice}

In the current version, three analyzing modules are in use: a spell
checker, a correctness checker, and an abbreviation checker.

The configuration format for entry level configuration uses a small
subset of the configuration specified.  It adds the
\texttt{OPTanalyze} tag and the configuration \texttt{@OK} and
\texttt{@LANG}.

The spellchecker module runs \newdef{aspell} in the background to do
the spell check.  Currently, the spell checker module is limited to
titles only.  When spell checking, it uses the configuration to
determine the language, \eg, \texttt{OPTanalyze = "@LANG=DA"}.

The correctness checker verifies the conformity with the {\bibtex}
specification and a few known de-facto standards.  Currently, the
format for specifying entry rules is JSON, but the {\bibtex} based
format described in Section~\ref{sec:analyzing_in_practice} would have
been better.  The format in use is essentially the JSON equivalent of
the one specified earlier.  \figref{fig:correctness_checker_json}
displays a snippet of the configuration.

\begin{figure}
  \centering
\begin{minted}{json}
{
  "book": {
    "author": {
      "required": true,
      "excludes": "editor"
    },
    "editor": {
      "required": true,
      "excludes": "author"
    },
    [...]
  }
}
\end{minted}
\caption{A snippet of the JSON for configuring the correctness checker}
\label{fig:correctness_checker_json}
\end{figure}

In {\orangutan}, the de-facto standards added are \texttt{crossref},
\texttt{issn}, \texttt{doi}, \texttt{oldforum} and \texttt{opt*}, the
last one using the wildcard to match all tags that have been commented
out.  All the de-facto standards are accepted on \texttt{*} entries,
again using the wildcard to match all entry types.

The abbreviation checker runs through journal names using a known list
of abbreviations.  The detection can be improved by detecting
abbreviations that are not on the list, using known standards for how
to abbreviate and detecting the various ways people abbreviate.
Furthermore, it can be improved by detecting if the journal name is
written with text rather than using a {\bibtex} string.


\section{Summary and conclusions}

Analyzing {\bibtex} files can be done using the two phases suggested,
starting with the lexical and correctness concerns, followed by the
consistency concerns.

To find the lexical and correctness issues, the following can be done:
running a spell check on entries, checking author names in databases,
detecting initials using single letters optionally followed by a
punctuation, prioritizing trusted sources for online lookups, using
rules to check conformity with the standards, using patterns to verify
values, using databases of known abbreviations to detect
abbreviations, and detecting when names of strings appear inside text.

To find the consistency issues: entries should be compared to find
duplicate content, to find changes in forum names, entries from the
same forum should be compared to detect inconsistent tag usage, and a
pattern should be used to detect inconsistent entry keys.

For the analysis, a configuration is provided to prevent false
positives.  A format for configuring has been introduced, keeping as
close to the {\bibtex} format as possible.  For entry level
configuration by introducing two de-facto standards, and for general
configuration by introducing an approximation to a {\bibtex} file,
using additional wildcard notations, strings with special meanings and
entry key naming to disambiguate the purpose of the configuration.

A prototype, {\orangutan}, has been introduced, showing a proof of
concept that the analysis can be done and that de-facto standards can
be used for configuration.

Having a tool that can analyze {\bibtex} files, and finding the issues
in the \file{bib} is the first step in handling the issues.  However,
just like the lookup services can lure a user into a false sense of
security, so can the result of an analysis.  Dijkstra's famous quip
``testing only shows the presence of bugs, not their
absence''~\cite{buxton1970_software} also applies to this analysis
tool: it can only reveal the issues that are tested for.  If it does
not find anything, it does not mean that there are no issues.  A final
question remains though: what should we do with the issues that the
analysis tool detects?  This is the topic of the next chapter.
