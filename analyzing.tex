\rfquote{Testing shows the presence, not the absence of bugs.}{Edsger
  W. Dijkstra}


\section{Introduction}

The goal of this chapter is to show how {\bibtex} files are analyzed.
Covering: what should be done about {\bibtex} in principle and
practice \chapref{sec:analyzing_what_to_do} and an analyzing prototype
named {\orangutan} \chapref{sec:analyzing_orangutan}.


\section{What should be done about {\bibtex}}
\label{sec:analyzing_what_to_do}
\subsection{In principle}

Due to the practical issues in changing/replacing {\bibtex} and to
ensure separation of concerns an analyzing tool should be an
augmenting tool.

To analyze {\bibtex} one would need to parse a \file{bib} into a
suitable representation.  This representation would need to parse
{\bibtex} entries and strings at a minimum.  If one intents to pretty
print the result after resolving all issues, the parsing also needs
the preambles.  Comments are technically optional, but should be kept
too.

The easiest approach is a two step parse: first taking care of the
lexical and correctness concerns, then the consistency concerns.  By
taking care of the lexical and correctness concerns first the optimal
conditions for taking care of the consistency concerns is made.

The term database i here use for both local and online databases,
since it does not matter if one use a local copy, if it is up to date.

\subsubsection{First step: Lexical and correctness}
\begin{itemize}
\item Spelling errors in general, should be detected either by online
  lookups or a spell checker.  A combination of the two could be
  powerful, since it gives a potential indicator of false positives.
  Using a spell checker requires a way to configure the language for
  the individual entries.

\item Spelling errors in names should use databases with
  names for detection.

\item Initials can be detected by finding cases with a single letter,
  perhaps followed by a punctuation.  For suggestions databases with
  names will be useful.  Handling multiple letters combined can be
  done by looking for relatively few (probably up to 3) letters that
  are all capitalized.

\item Online lookups, will be impossible to detect for certain,
  because the intended search will be unknown.  The detection of the
  other issues can give an indicator of a bad lookup.  Since online
  lookups is a likely way to handle the databases needed for this
  tool, the quality of a lookup is a concern.  The most trusted online
  database available should be used whenever possible.  A system doing
  lookups in multiple databases will give further indications of
  potential issues.  The results should always be confirmed by the
  user, to prevent erroneous data.

\item Conformity to de-facto standards and the {\bibtex} specification
  requires a set of rules specifying: required tags, optional tags,
  exclusive tags, and inclusive tags.

\item Detecting invalid values should be done where there are clear
  rules for a correct value can be specified: such as ISSN, year, and
  month.  Furthermore, values that can be verified using a database
  should also be verified.

\item To detect journal names, in abbreviated form, or in full form,
  is done using a database of know journal names and their
  abbreviations.  It should use a database to de-abbreviate or
  abbreviate.  Furthermore this can be refined by detecting unknown
  abbreviations.

\item To handle {\bibtex} strings that end up as part of the text, the
  text should be compared with the strings in the \file{bib}.
\end{itemize}


\subsubsection{Second step: Consistency}
\begin{itemize}
\item To detect duplicate entries, using unique identifiers is the
  best way, then based on identical entries.  Otherwise potential
  duplicates is detected by a specification of similarity: having the
  title and authors as primary indicators.  For duplicated values it
  should use the strings if possible to determine when the same source
  is referenced.  For the duplicated values is should build a list of
  values that are likely to be repeated (such as journal names) and
  use that to detect when textual content are duplicated.

\item To detect name changes of forums, a database of known changes is
  used, in conjunction with a way of specifying the changes.

\item Inconsistent tag usage should be handled by mapping entries from
  the same forums and comparing the tags in use for missing and
  additional tags.  Comparing forum entries over time will increase
  the usefulness, but adds the need to handle changes in the tags in
  use over time.

\item Inconsistent entry keys should be handled by having a naming
  scheme based on the data in the entries, with a way to disambiguate
  if two different entries would get the same name.
\end{itemize}


\subsubsection{Configuration}
\label{sec:analyzing_configuration}

Both of the steps above require ways to configure the behavior.  The
use of configurations should be as close to {\bibtex} as possible.

The preference to using the {\bibtex} format allows people to use what
they are familiar with.  Using a format that are readily supported in
programming frameworks, like JSON or XML might be considered easy by
the computer scientist, especially if it is one used to working with
those formats.  However, people outside computer science, such as a
physicist or the helpful chemist from earlier, will likely not be
familiar with such formats, nor should they.  A user of {\bibtex}
should at best only be concerned with {\bibtex}, when working with
{\bibtex}.

To use something in anger is an idiom for if something has been tested
in practice.  The idea of coding in anger has been expressed by Philip
Wadler~\cite{wadler1997_functional}.  For the {\bibtex} user, coding
in anger, could be the situation where the deadline is getting closer
and one just need things to work, at best ten minutes ago.  When faced
with the frustrations like that one tend not to care about beautiful
and elegant solutions, but rather wanting something that works with a
minimum of effort.  To accommodate the user writing {\bibtex} in
anger, is another reason to keep the specification as close to
{\bibtex} as possible, since it will minimize the effort.

Configuration should be done via de-facto standards inside the
\file{bib}, whenever possible.  For some configurations, de-facto
standards inside the \file{bib} is unreasonable.  These configurations
is better put into separate files.  However, the specification of
should still be designed to match {\bibtex} as closely as possible,
\ie, still using entries, tags and values to configure.


\subsection{In practice}
\label{sec:analyzing_in_practice}

\subsubsection{Entry level configurations}

To account configure the analysis, introducing two de-facto
standards will be appropriate: \texttt{OLDforum} to mark a previous
name of a forum, and \texttt{OPTanalyze} to configure tell the tool
about entry specific details.

The division into two de-facto standards are done for two reasons: for
\texttt{OLDforum} the additional standard will allow bibliography
styles to make use of the additional information (one could easily
imaging a bibliographic style write ``\texttt{NISSC \textit{formerly
    known as} NCSC}''), and for \texttt{OPTanalyze} the content is
considered unlikely to be relevant to print in a bibliography.
Furthermore, the settings for \texttt{OPTanalyze} is kept in one tag
to prevent a multitude of new tags.

\remark{Olivier: Yikes, is there any way to make the following
  clearer?}

For the \texttt{OLDforum} tag, the value should be the string
containing the old name for the forum.  In some cases a forum can have
multiple name changes.  When multiple name changes occurs, referring
the most recent name in the \file{bib} is desired.  The tag is
intended for disambiguation within a given file, not all files in
general.  If the tag is used for a bibliography style having a
multitude of names as the value will likely be confusing.

However, if a forum name has been omitted, because no entries with
that name was in the \file{bib}, then a re-detection of name changes
will be needed if this name is added later.  Furthermore, using it in
bibliography styles may cause issues if we only desire to show the
previous name, when it is used in the references, however, since the
tag is presently not in use in any bibliography style, these issues
are not considered further.  Alternatively the format could be a list
of names (comma separated, since {\bibtex} use a comma as a
separator), this list would allow detailed backtracking.  A design
with detailed backtracking

Defining entry level deviations is done using \texttt{OPTanalyze},
using spaces to separate multiple settings.  The values for desired
deviations is as follows:

\begin{itemize}
\item \texttt{@DUPLICATEOK=X} to specify that an entry marked as a
  potential duplicate if deliberate, replacing \texttt{X} with the
  entry key of the potential duplicate.
\item \texttt{@LANG=XX} to specify the desired spell check language,
  replacing \texttt{XX} with the language code desired.
\item \texttt{@SPELLINGOK} to mark the spelling as correct.
\item \texttt{@NAMESOK} to mark that the names are correct.
\item \texttt{@INITIALSOK} to mark that the initials are correct.
\item \texttt{@NOLOOKUP} to mark that no look up should be done for
  the content of this entry.
\item \texttt{@CONFORMITYOK} to mark conformity to the specification
  and de-facto standards as correct.
\item \texttt{@ABBREVIATIONOK} to mark an abbreviated form as correct.
\item \texttt{@STRINGSOK} to mark that the texts has been checked for
  strings and that is is correct.
\item \texttt{@TAGSOK=forum} to mark that the usage of tags is correct and
  defines the standard for tag use for the entries from the \emph{same
  occurrence} of the forum.
\item \texttt{@TAGSOK=future} to mark that the usage of tags is
  correct and defines the standard for tag use for the entries from
  the \emph{same and future occurrences} of the forum.
\item \texttt{@TAGSOK=single} to mark that the usage of tags is
  correct for this single entry, not affecting other entries from the
  forum.
\item \texttt{@ENTRYKEYOK} to mark the entry key as correct.
\item \texttt{@LEXICALLYOK} to ignore all lexical checks for the
  entry, should be used with care.
\item \texttt{@CONSISTENCYOK} to ignore all consistency checks for the
  entry, should be used with care.
\item \texttt{@OK}, to mark an entry as fully correct, same as
  \texttt{@CONFORMITYOK @LEXICALLYOK @CONSISTENCYOK}, should be used
  with care.
\end{itemize}

The settings: \texttt{@ABBREVIATIONOK}, \texttt{@LEXICALLYOK},
\texttt{@CONSISTENCYOK}, and \texttt{@OK} may be a bit debatable on
necessity, however, their presence do ensure that the configuration is
complete and consistent.

For the conformity and de-facto analysis, a consideration would be to
have configurations, for explicitly specifying explicitly specifying
that which deviations that are being accepted.  For instance,
specifying that we accept a missing title on one of the articles.
However, explicitly allowing and denying tags will likely be
redundant, since once the deviation has been accepted one will not be
likely to change that.

A similar set of tags could also be defined for the consistency check.
For the consistency check, it can be argued that an entry not
conforming to the standards of a forum may be updated to do so.  For
instance, if the norm for a forum is to have an ISSN on all entries,
and some of the entries do not have an ISSN.  Those entries might get
an ISSN assigned later, which we at the time add to the entry.  In the
case of adding the missing information, the \texttt{@CONSISTENCYOK}
configuration will become redundant.  By knowing which tags caused the
addition of \texttt{@CONSISTENCYOK}, one can remove the redundant
configuration.  Another approach to configurations becoming redundant
is to analyze whether the configurations have any effect on the
analysis.  This approach is considered more appropriate, because of
the simplicity for the user.

Another potential change is: to have a configuration for trusted
lookup services.  For example, if one knows that an entry is correct
in a certain database, then specifying that this database is trusted
for that entry.  This configuration might lead to a false sense of
security, since there is no way to guarantee that the data will never
be corrupted in that database.

An example of the configurations can be seen in
\figref{fig:analyzing_added_de_facto_standards}

\begin{figure}
  \centering
\begin{verbatim}
@BOOK{blendstrup1994Mistbaenk,
  author = "Jens Blendstrup",
  title = "Mennesker i En Mistb{\ae}nk",
  year = 1994,
  OPTanalyze = "@LANG=DA @NAMESOK @CONFORMITYOK"
}
\end{verbatim}
  \caption{An example using the de-facto standards for configuration,
    setting the spell checking language to Danish, accepting the name
    ``Jens Blendstrup'' and ignoring the missing publisher.}
  \label{fig:analyzing_added_de_facto_standards}
\end{figure}

To make the configurations even more intuitive for a {\bibtex} user,
an option is to add {\bibtex} strings with the relevant options in the
top of one's \file{bib}.  Then when configuring one can just use the
{\bibtex} strings and concatenate the relevant configurations.  An
example of configuration with strings can be seen in
\figref{fig:analyzing_added_de_facto_standards_strings}.  These
strings would then have to be added in the top of the \file{bib}, so
{\bibtex} will not complain.

\begin{figure}
  \centering
\begin{verbatim}
@BOOK{blendstrup1994Mistbaenk,
  author = "Jens Blendstrup",
  title = "Mennesker i En Mistb{\ae}nk",
  year = 1994,
  OPTanalyze = analyze_lang_danish # analyze_names_ok # analyze_conformity_ok
}
\end{verbatim}
  \caption{\figref{fig:analyzing_added_de_facto_standards_strings}
    rewritten to use strings for configuration.}
  \label{fig:analyzing_added_de_facto_standards_strings}
\end{figure}


\subsubsection{Bibliography level configurations}

For the configurations desired, some are not specific to an entry, but
rather the entire bibliography.  Two options for these configurations
are: to put such options inside one's \file{bib}, or put them in
separate files.  Adding them to one's \file{bib} will introduce an
additional mess in the file, which is counterproductive since the goal
is to clean up the mess.  Furthermore, the configurations may not
correspond to proper {\bibtex} formatting.  Having the configurations
in separate files, provides: separation of concerns, a clean
\file{bib} and allows deviations from {\bibtex} if needed.
Furthermore, having configurations in separate files allows sharing of
the files, for instance, if a publisher want their authors to follow a
certain setup.

Such configuration files should still, if possible, follow the
{\bibtex} format.  Thus using entries with tags and values for
configuration and for configurations where entries and tags do not
make sense, defining {\bibtex} strings will be the favored choice.

For changes in names of forums a database of such changes is needed.
Currently no such database exist (to the authors knowledge), so the
user will need a way to specify his own.  Even if such a database did
exist a way to configure name changes is still desired, since the
database might not be complete.  A name change can be specified by
having two entries for the desired forum, adding the \texttt{OLDforum}
tag, marking the name change.  An example of a name change
configuration can be seen in
\figref{fig:analyzing_configuration_name_change}.

\begin{figure}
  \centering
\begin{verbatim}
@PROCEEDINGS{forum_ncsc,
  title = "National Computer Security Conference"
}

@PROCEEDINGS{forum_nissc,
  title = "National Information Systems Security Conference",
  OLDforum = "ncsc_forum"
}
\end{verbatim}
  \caption{Configuring a name change of a forum}
  \label{fig:analyzing_configuration_name_change}
\end{figure}

However, this configuration ignore that the names in the actual
\file{bib} may be in their abbreviated form.  Some forums also have,
as part of the name, text identifying which instance of the forum it
is.  For example, in the example above an entry would be named
something like \texttt{Proceedings of the 20th National Information
  Systems Security Conference} and not just \texttt{National
  Information Systems Security Conference} as in the example.  In
stead of writing the name as a text a better way might be to use
strings.  If the \file{bib} construct forum names using strings, as in
\figref{fig:analyzing_configuration_name_change_bib_file_strings}.
The configuration can reuse the string for identifying the forum
(\texttt{nissc} in the example), this configuration will allow the
analysis to detect that it is the same string, and thus enable it to
detect that it is the same forum.  The corresponding configuration
will look like
\figref{fig:analyzing_configuration_name_change_config_file_strings}.

\begin{figure}
  \centering
\begin{small}
\begin{verbatim}
% Re-usable strings
@STRING{PROCintro = "Proceedings of the"}
@STRING{nissc = "National Information Systems Security Conference"}

% Conferences
@STRING{nissc20 = PROCintro # "20th" # nissc}

% Proceedings
@INPROCEEDINGS{porras1997emerald,
  title = "EMERALD: Event monitoring enabling response to anomalous live disturbances",
  author = "Porras, Phillip A and Neumann, Peter G",
  booktitle = nissc20
}
\end{verbatim}
\end{small}
  \caption{\file{bib} using strings for conference names}
  \label{fig:analyzing_configuration_name_change_bib_file_strings}
\end{figure}

\begin{figure}
  \centering
\begin{verbatim}
@PROCEEDINGS{forum_nissc,
  title = nissc,
  OLDforum = "forum_ncsc"
}
\end{verbatim}
  \caption{\file{bib} using strings for conference names}
  \label{fig:analyzing_configuration_name_change_config_file_strings}
\end{figure}

\remark{Olivier: The following paragraph feels weak to me, do you have
  a suggestion for how to explain this in a clearer way?}

The configuration of name changes should be using the most general
entry type available, such as \texttt{@ARTICLE}, \texttt{@PROCEEDINGS}
and \texttt{@BOOK}.  The name change analysis recognize and map the
general entry types to the specific types.  For instance, recognizing
that \texttt{booktitle} in \texttt{@INPROCEEDINGS} corresponds to the
\texttt{title} in a \texttt{@PROCEEDINGS}.

Specifying the de-facto standards is done using {\bibtex} entries, and
only deviations should be specified.  The configurations correspond to
the rules in Section~\ref{sec:about_micro_use}, with the addition of
the option to refuse a tag.  The configurations are:

\begin{itemize}
\item \texttt{@REQUIRED} for a tag we require to be present in the
  entry type.
\item \texttt{@OPTIONAL} for optional tags.
\item \texttt{@DENY} for tags that are in the default configuration,
  that we want to reject.
\item \texttt{@EXLUDES=tag} for a tag that excludes the use of another
  tag, replacing \texttt{tag} with the name of another tag.  For
  example, if one allows both \texttt{ISSN} and \texttt{DOI} as tags,
  but want to ensure that only one of the tags is present, one would
  have the following: \texttt{ISSN = "@REQUIRED @EXLUDES=DOI"} and
  \texttt{DOI = "@REQUIRED @EXCLUDES=ISSN"}.
\item \texttt{@INCLUDES=tag} for a tags where one of them is required
  and the other optional.  Usage is similar to \texttt{@EXCLUDES=tag}.
\end{itemize}

An example of a configuration of standards can be seen in
\figref{fig:analyzing_standards_config}.  This example sets article
entries to: reject \texttt{address} tags, that either \texttt{DOI} or
\texttt{ISSN} is present (but not both) and adds \texttt{url} as an
optional tag.  For books entries the example sets: that ISBN10 and/or
ISBN13 must be present.

\begin{figure}
  \centering
\begin{verbatim}
@ARTICLE{standards_article,
  address = "@DENY",
  DOI = "@REQUIRED @EXCLUDES=ISSN",
  ISSN = "@REQUIRED @EXCLUDES=DOI",
  url = "@OPTIONAL"
}

@BOOK{standards_book,
  ISBN10 = "@REQUIRED @INCLUSIVE=ISBN13",
  ISBN13 = "@REQUIRED @INCLUSIVE=ISBN10"
}
\end{verbatim}
  \caption{A snippet of the desired {\bibtex} based configuration for
    the correctness checker}
  \label{fig:analyzing_standards_config}
\end{figure}

The configuration of standards also allows usage of a \texttt{*} as a
\newdef{wildcard}.  The wildcard will then match anything, for example
\texttt{@*PROCEEDINGS} will match \texttt{@PROCEEDINGS},
\texttt{@INPROCEEDINGS} and any entry type that one introduces that has
a name ending in proceedings.  These wildcards can be used for both
tag names and entry types.

Abbreviations of journal names can be configured by, adding
\texttt{@ARTICLE} entries, using the two tags: \texttt{abbreviated}
and \texttt{fullname}, specifying the abbreviated journal name and
full journal name respectively.

% Bah, proceedings are not as easy ><
\remark{Still need to specify something for consistency changes and if
  a tag is the desired consistency... If the one in OPTanalyze doesn't
  actually cover this sufficiently?}

For the specification of entry keys using a {\bibtex} string with the
predefined name \texttt{ENTRY\_KEY}.  Inside the string some way of
specifying the desired template for entry keys is needed.  Using a
template scheme, such as \texttt{\{tag\}} to match tags, is probably
the best solution.  This template system contradicts the desire to
keep the format close to {\bibtex}, but no better way has been found.
A template could look like: \texttt{\{author\}\{year\}\{title\}}.

However, this template is insufficient for two reasons: spaces are not
allowed in the entry key, and when people name entries they often use
names such as: only use the last name of the first author in the list
and one significant word from the title.  Refining the templates to
allow \newdef{selectors} before the fields, such as: selecting the
first part of an entry, last name, first name, and significant word,
will improve the usability.  For example,
\texttt{[lastname]\{author\}} would select the last name of the
authors.  Using these selectors in conjunction to refine the result,
for instance, \texttt{[lastname][first]\{author\}}.  Again this moves
the format away from how {\bibtex} is defined and no better solution
has been found.

Fortunately, {\bibtex} strings comes to the rescue - at least
partially.  Having strings for the most common matches will ensure
that most users will never need to see, nor even know about, the
underlying pattern matching system.  Using strings will allow the user
to use concatenations to build the desired pattern.  And introducing
an empty string named \texttt{OF} and one named \texttt{THEN} to
support a more natural language.  And example can be seen in
\figref{fig:analyzing_entry_key_pattern}.

\begin{figure}
  \centering
\begin{verbatim}
@STRING{ENTRY_KEY = LASTNAME # OF # FIRST # AUTHOR # THEN # YEAR}
\end{verbatim}
  \caption{An example of a entry key pattern using the first name of
    the first year, followed by the year.}
\label{fig:analyzing_entry_key_pattern}
\end{figure}

The strings and selectors are:

\begin{itemize}
\item \texttt{FIRST} the first part of a tag value, if the tag data is
  separated by \texttt{and}, like a list of authors, the first part of
  this list should be selected, otherwise select the first word.  The
  corresponding selector \texttt{[first]}.
\item \texttt{LAST} the last part of a tag value, if the tag data is
  separated by \texttt{and}, like a list of authors, the last part of
  this list should be selected, otherwise select the last word.  The
  corresponding selector \texttt{[last]}.
\item \texttt{FIRSTPART} selects the first part of a tag value, if the
  tag data is separated by \texttt{and}, like a list of authors, the
  first word in each part of the list should be selected, otherwise
  just the first word.  The corresponding selector
  \texttt{[firstpart]}.
\item \texttt{LASTPART} selects the last part of a tag value, if the
  tag data is separated by \texttt{and}, like a list of authors, the
  last word in each part of the list should be selected, otherwise
  just the last word.  The corresponding selector \texttt{[lastpart]}.
\item \texttt{FIRSTNAME} same as \texttt{FIRSTPART}, included to allow
  a more natural language.
\item \texttt{LASTNAME} same as \texttt{LASTPART}, included to allow
  a more natural language.
\item \texttt{SIGNIFICANT} selects the significant words of a
  sentence, by removing anything that is not nouns.  The
  corresponding selector \texttt{[significant]}.  \remark{Olivier:
    is there a better choice?}
\item \texttt{OF} and \texttt{THEN} empty placeholders, included to
  allow a more natural language.
\end{itemize}

The templates for tags are:

\begin{itemize}
\item \texttt{AUTHOR} the content of the author or editor tag.  The
  corresponding template \texttt{[author]}
\item \texttt{FORUM} the tag containing the relevant forum, such as:
  journal name, conference name and publisher.  The corresponding
  template \texttt{[forum]}.
\item \texttt{TITLE} the content of the title tag.  The corresponding
  template \texttt{[title]}
\item \texttt{YEAR} the content of the year tag.  The corresponding
  template \texttt{[year]}
\end{itemize}

One can introduce new tags in this manner and define appropriate
strings.  A way of expanding the selectors is desired, but would
complicate things.  Rules for handling empty tags and alternative
actions would be useful, however, this would complicate things even
further.


\section{{\orangutan}}
\label{sec:analyzing_orangutan}
\subsection{Introduction}

A prototype for some of the analysis, has been made under the name
{\orangutan}.

\remark{Olivier: would it be appropriate to mention that the name
  {\orangutan} is a reference to The Librarian (he is never mentioned
  by name) from Discworld, who happens to be an Orangutan}


\subsection{Why {\orangutan} came to be}

There are a lot of tools that provide partial solutions, so having an
augmenting tool would be an improvement.  The tool will be most
effective if it address first the correctness and lexical concerns
then the consistency concerns.  The attempt at making this tool has
been named \newdef{\orangutan}.


\subsection{What is {\orangutan}}

In the same spirit as {\bibtex}, {\orangutan} is designed to be a
simple software tool to help one in improving bibliographic
references.  The analysis is designed over the same principles as in
Section~\ref{sec:analyzing_what_to_do}.


\subsection{How {\orangutan} is used in principle}

When analyzing {\bibtex} files {\orangutan} operates on the first step
of the analysis: correctness and lexical concerns.  The tool use
options, set by introducing a de-facto standard.  The options are for
specifying if the language for the spell check and that the entry is
already considered to be correct.  To make the options intuitive for
the {\bibtex} user the options are designed to match the {\bibtex}
format.


\subsection{How {\orangutan} is used in practice}

In the current version there are three analyzing modules in use: a
spell checker, a correctness checker and an abbreviation checker.

The configuration format for entry level configuration use a small
subset of the configuration specified.  It adds the
\texttt{OPTanalyze} tag and the configuration \texttt{@OK} and
\texttt{@LANG}.

The spell checker runs a spell check in the background using aspell.
Currently the spell checker is limited to only titles.  When spell
checking it use the configuration to specify the language, \eg,
\texttt{OPTanalyze = "@LANG=DA"}.

The correctness checker verifies the conformity with the {\bibtex}
specification and a few known de-facto standards.  Currently the
format for specifying entry rules is JSON, the {\bibtex} based format
described in Section~\ref{sec:analyzing_in_practice} would have been
better.  The format in use is essentially the JSON equivalent of the
one specified earlier, a snippet of the configuration can be seen in
\figref{fig:correctness_checker_json}.

\begin{figure}
  \centering
\begin{minted}{json}
{
  "book": {
    "author": {
      "required": true,
      "excludes": "editor"
    },
    "editor": {
      "required": true,
      "excludes": "author"
    },
    [...]
  }
}
\end{minted}
\caption{A snippet of the JSON for configuring the correctness checker}
\label{fig:correctness_checker_json}
\end{figure}

The abbreviation checker runs through journal names using a known list
of abbreviations.  The detection can be improved by detecting
abbreviations that are not on the list, using know standards for how
to abbreviate and detecting the various ways people abbreviate.
Furthermore, it can be improved by detecting if the journal name is
written with text rather than using a {\bibtex} string.


\section{Summary and conclusions}

To summarize:

Having a tool that can analyze {\bibtex} files, and find the issues in
the \file{bib} is the first step in handling the issues.  However,
just like the lookup services can lure a user into a false sense of
security, so can the result of an analysis.  The famous quote:
``testing only shows the presence of bugs, not their absence'', also
holds for this analysis tool, it can only reveal the issues that are
tested for, it if does not find anything, it does not mean that there
is no issues.  A final question remain though: what should we do with
the issues that the analysis tool detects?
